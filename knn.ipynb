{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Flowers Using Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is from: https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/\n",
    "\n",
    "The test problem we will be using in this tutorial is iris classification.\n",
    "\n",
    "Based on the sepal length, sepal width, petal length and petal width of 150 observations of iris flowers, we need to implement a classification model to predict the species, which is one of setosa, versicolor or virginica.\n",
    "\n",
    "It is a standard dataset where the species is known for all instances. As such we can split the data into training and test datasets and use the results to evaluate our algorithm implementation. Good classification accuracy on this problem is above 90% correct, typically 96% or better.\n",
    "\n",
    "Here is a iris flower image:\n",
    "<img src=\"iris_petal_sepal.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to implement k-Nearest Neighbors in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is broken down into the following steps:\n",
    "\n",
    "1. Handle Data: Open the dataset from CSV and split into test/train datasets.\n",
    "2. Similarity: Calculate the distance between two data instances.\n",
    "3. Neighbors: Locate k most similar data instances.\n",
    "4. Response: Generate a response from a set of data instances.\n",
    "5. Accuracy: Summarize the accuracy of predictions.\n",
    "6. Main: Tie it all together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Handle Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is load our data file. The data is in CSV format without a header line or any quotes. We can open the file with the open function and read the data lines using the reader function in the csv module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot convert numpy.ndarray to numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      3\u001B[0m url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://raw.githubusercontent.com/ruiwu1990/CSCI_4120/master/KNN/iris.data\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 4\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(df\u001B[38;5;241m.\u001B[39mhead())\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m   1014\u001B[0m     dialect,\n\u001B[0;32m   1015\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m   1023\u001B[0m )\n\u001B[0;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[1;32m--> 626\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1968\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m   1965\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1966\u001B[0m         new_col_dict \u001B[38;5;241m=\u001B[39m col_dict\n\u001B[1;32m-> 1968\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1969\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnew_col_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1970\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1971\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1972\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43musing_copy_on_write\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1973\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1975\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_currow \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m new_rows\n\u001B[0;32m   1976\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001B[0m, in \u001B[0;36mDataFrame.__init__\u001B[1;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[0;32m    772\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_mgr(\n\u001B[0;32m    773\u001B[0m         data, axes\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m\"\u001B[39m: index, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: columns}, dtype\u001B[38;5;241m=\u001B[39mdtype, copy\u001B[38;5;241m=\u001B[39mcopy\n\u001B[0;32m    774\u001B[0m     )\n\u001B[0;32m    776\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m    777\u001B[0m     \u001B[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001B[39;00m\n\u001B[1;32m--> 778\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[43mdict_to_mgr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtyp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmanager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    779\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ma\u001B[38;5;241m.\u001B[39mMaskedArray):\n\u001B[0;32m    780\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mma\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mrecords\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:443\u001B[0m, in \u001B[0;36mdict_to_mgr\u001B[1;34m(data, index, columns, dtype, typ, copy)\u001B[0m\n\u001B[0;32m    440\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m columns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    441\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mseries\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Series\n\u001B[1;32m--> 443\u001B[0m     arrays \u001B[38;5;241m=\u001B[39m \u001B[43mSeries\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mobject\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    444\u001B[0m     missing \u001B[38;5;241m=\u001B[39m arrays\u001B[38;5;241m.\u001B[39misna()\n\u001B[0;32m    445\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    446\u001B[0m         \u001B[38;5;66;03m# GH10856\u001B[39;00m\n\u001B[0;32m    447\u001B[0m         \u001B[38;5;66;03m# raise ValueError if only scalars in dict\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:490\u001B[0m, in \u001B[0;36mSeries.__init__\u001B[1;34m(self, data, index, dtype, name, copy, fastpath)\u001B[0m\n\u001B[0;32m    487\u001B[0m name \u001B[38;5;241m=\u001B[39m ibase\u001B[38;5;241m.\u001B[39mmaybe_extract_name(name, data, \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m))\n\u001B[0;32m    489\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 490\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[43mensure_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    492\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    493\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_dtype(dtype)\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7647\u001B[0m, in \u001B[0;36mensure_index\u001B[1;34m(index_like, copy)\u001B[0m\n\u001B[0;32m   7645\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m MultiIndex\u001B[38;5;241m.\u001B[39mfrom_arrays(index_like)\n\u001B[0;32m   7646\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 7647\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mIndex\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex_like\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtupleize_cols\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   7648\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   7649\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Index(index_like, copy\u001B[38;5;241m=\u001B[39mcopy)\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:565\u001B[0m, in \u001B[0;36mIndex.__new__\u001B[1;34m(cls, data, dtype, copy, name, tupleize_cols)\u001B[0m\n\u001B[0;32m    562\u001B[0m         data \u001B[38;5;241m=\u001B[39m com\u001B[38;5;241m.\u001B[39masarray_tuplesafe(data, dtype\u001B[38;5;241m=\u001B[39m_dtype_obj)\n\u001B[0;32m    564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 565\u001B[0m     arr \u001B[38;5;241m=\u001B[39m \u001B[43msanitize_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    566\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m    567\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex must be specified when data is not list-like\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(err):\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\.venv\\Lib\\site-packages\\pandas\\core\\construction.py:654\u001B[0m, in \u001B[0;36msanitize_array\u001B[1;34m(data, index, dtype, copy, allow_2d)\u001B[0m\n\u001B[0;32m    651\u001B[0m     subarr \u001B[38;5;241m=\u001B[39m _try_cast(data, dtype, copy)\n\u001B[0;32m    653\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 654\u001B[0m     subarr \u001B[38;5;241m=\u001B[39m \u001B[43mmaybe_convert_platform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    655\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m subarr\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mobject\u001B[39m:\n\u001B[0;32m    656\u001B[0m         subarr \u001B[38;5;241m=\u001B[39m cast(np\u001B[38;5;241m.\u001B[39mndarray, subarr)\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\.venv\\Lib\\site-packages\\pandas\\core\\dtypes\\cast.py:139\u001B[0m, in \u001B[0;36mmaybe_convert_platform\u001B[1;34m(values)\u001B[0m\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m _dtype_obj:\n\u001B[0;32m    138\u001B[0m     arr \u001B[38;5;241m=\u001B[39m cast(np\u001B[38;5;241m.\u001B[39mndarray, arr)\n\u001B[1;32m--> 139\u001B[0m     arr \u001B[38;5;241m=\u001B[39m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaybe_convert_objects\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m arr\n",
      "File \u001B[1;32mlib.pyx:2538\u001B[0m, in \u001B[0;36mpandas._libs.lib.maybe_convert_objects\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: Cannot convert numpy.ndarray to numpy.ndarray"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "    print(\"Pandas is installed. Version:\", pd.__version__)\n",
    "except ImportError:\n",
    "    print(\"Pandas is not installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Cannot convert numpy.ndarray to numpy.ndarray\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to the iris.data file\n",
    "file_path = r'/KNN/iris.data'\n",
    "\n",
    "# Define column names for clarity\n",
    "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "\n",
    "try:\n",
    "    # Load the dataset without specifying separator, allowing pandas to infer it\n",
    "    df = pd.read_csv(file_path, header=None, names=column_names, encoding='utf-8')\n",
    "\n",
    "    # Display the first few rows of the dataframe\n",
    "    print(\"First few rows of the dataframe:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Check for any missing values or data type issues\n",
    "    print(\"\\nDataframe info:\")\n",
    "    print(df.info())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to split the data into a training dataset that kNN can use to make predictions and a test dataset that we can use to evaluate the accuracy of the model.\n",
    "\n",
    "We first need to convert the flower measures that were loaded as strings into numbers that we can work with. Next we need to split the data set randomly into train and datasets. A ratio of 67/33 for train/test is a standard ratio used.\n",
    "\n",
    "Pulling it all together, we can define a function called loadDataset that loads a CSV with the provided filename and splits it randomly into train and test datasets using the provided split ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "def loadDataset(filename, split):\n",
    "\ttrainingSet=[]\n",
    "\ttestSet=[]\n",
    "\tdf = pd.read_csv(url, header=None)\n",
    "\tarray = df.to_numpy()\n",
    "\trandom.shuffle(array)\n",
    "\ttraining_len = int(len(array)*split)\n",
    "\ttrainingSet = array[:training_len]\n",
    "\ttestSet = array[training_len:]\n",
    "\treturn trainingSet, testSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this function out with our iris dataset, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 99\n",
      "Test: 51\n"
     ]
    }
   ],
   "source": [
    "trainingSet=[]\n",
    "testSet=[]\n",
    "url = 'https://raw.githubusercontent.com/ruiwu1990/CSCI_4120/master/KNN/iris.data'\n",
    "trainingSet, testSet = loadDataset(url, 0.66)\n",
    "\n",
    "# repr() Return a string containing a printable representation\n",
    "print('Train: ' + repr(len(trainingSet)))\n",
    "print('Test: ' + repr(len(testSet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make predictions we need to calculate the similarity between any two given data instances. This is needed so that we can locate the k most similar data instances in the training dataset for a given member of the test dataset and in turn make a prediction.\n",
    "\n",
    "Given that all four flower measurements are numeric and have the same units, we can directly use the Euclidean distance measure. This is defined as the square root of the sum of the squared differences between the two arrays of numbers (read that again a few times and let it sink in).\n",
    "\n",
    "Additionally, we want to control which fields to include in the distance calculation. Specifically, we only want to include the first 4 attributes. One approach is to limit the euclidean distance to a fixed length, ignoring the final dimension.\n",
    "\n",
    "Putting all of this together we can define the euclideanDistance function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def euclideanDistance(instance1, instance2, length):\n",
    "\tdistance = 0\n",
    "\tfor x in range(length):\n",
    "\t\tdistance += pow((instance1[x] - instance2[x]), 2)\n",
    "\treturn math.sqrt(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this function with some sample data, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 4.0\n"
     ]
    }
   ],
   "source": [
    "data1 = [2, 2, 2, 2, 'a']\n",
    "data2 = [4, 4, 4, 4, 'b']\n",
    "distance = euclideanDistance(data1, data2, 4)\n",
    "print('Distance: ' + repr(distance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a similarity measure, we can use it collect the k most similar instances for a given unseen instance.\n",
    "\n",
    "This is a straight forward process of calculating the distance for all instances and selecting a subset with the smallest distance values.\n",
    "\n",
    "Below is the getNeighbors function that returns k most similar neighbors from the training set for a given test instance (using the already defined euclideanDistance function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "def getNeighbors(trainingSet, testInstance, k):\n",
    "\tdistances = []\n",
    "\tlength = len(testInstance)-1\n",
    "\tfor x in range(len(trainingSet)):\n",
    "\t\tdist = euclideanDistance(testInstance, trainingSet[x], length)\n",
    "\t\tdistances.append((trainingSet[x], dist))\n",
    "    # key=operator.itemgetter(1) => order distances list based on second column (1)\n",
    "\tdistances.sort(key=operator.itemgetter(1))\n",
    "\tneighbors = []\n",
    "\tfor x in range(k):\n",
    "\t\tneighbors.append(distances[x][0])\n",
    "\treturn neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test out this function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 4, 4, 'b']]\n"
     ]
    }
   ],
   "source": [
    "trainSet = [[2, 2, 2, 'a'], [4, 4, 4, 'b']]\n",
    "testInstance = [5, 5, 5, 'b']\n",
    "k = 1\n",
    "neighbors = getNeighbors(trainSet, testInstance, k)\n",
    "print(neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have located the most similar neighbors for a test instance, the next task is to devise a predicted response based on those neighbors.\n",
    "\n",
    "We can do this by allowing each neighbor to vote for their class attribute, and take the majority vote as the prediction.\n",
    "\n",
    "Below provides a function for getting the majority voted response from a number of neighbors. It assumes the class is the last attribute for each neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def getResponse(neighbors):\n",
    "    # classVotes is a dict, similar to JSON\n",
    "    # e.g., {'a': 1, 'b': 2}\n",
    "\tclassVotes = {}\n",
    "\tfor x in range(len(neighbors)):\n",
    "        # based on the last element\n",
    "\t\tresponse = neighbors[x][-1]\n",
    "\t\tif response in classVotes:\n",
    "\t\t\tclassVotes[response] += 1\n",
    "\t\telse:\n",
    "\t\t\tclassVotes[response] = 1\n",
    "    # reverse=True means the most voted item will be our response\n",
    "    # itemgetter(1): get the first item, i.e. item with most votes\n",
    "\tsortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\treturn sortedVotes[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test out this function with some test neighbors, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "neighbors = [[1,1,1,1,'a'], [2,2,2,2,'a'], [3,3,3,3,'b']]\n",
    "response = getResponse(neighbors)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach returns one response in the case of a draw, but you could handle such cases in a specific way, such as returning no response or selecting an unbiased random response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all of the pieces of the kNN algorithm in place. An important remaining concern is how to evaluate the accuracy of predictions.\n",
    "\n",
    "An easy way to evaluate the accuracy of the model is to calculate a ratio of the total correct predictions out of all predictions made, called the classification accuracy.\n",
    "\n",
    "Below is the getAccuracy function that sums the total correct predictions and returns the accuracy as a percentage of correct classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(testSet, predictions):\n",
    "\tcorrect = 0\n",
    "\tfor x in range(len(testSet)):\n",
    "\t\tif testSet[x][-1] is predictions[x]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn (correct/float(len(testSet))) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this function with a test dataset and predictions, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.66666666666666\n"
     ]
    }
   ],
   "source": [
    "testSet = [[1,1,1,1,'a'], [2,2,2,2,'a'], [3,3,3,3,'b']]\n",
    "predictions = ['a', 'a', 'a']\n",
    "accuracy = getAccuracy(testSet, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the elements of the algorithm and we can tie them together with a main function.\n",
    "\n",
    "Below is the example of implementing the kNN algorithm from scratch in Python.\n",
    "\n",
    "Group Activity: You need to finish the TODO section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Example of kNN implemented from Scratch in Python\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrandom\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmath\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Example of kNN implemented from Scratch in Python\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import operator\n",
    "\n",
    "def loadDataset(filename, split):\n",
    "\ttrainingSet=[]\n",
    "\ttestSet=[]\n",
    "\tdf = pd.read_csv(url, header=None)\n",
    "\tarray = df.to_numpy()\n",
    "\trandom.shuffle(array)\n",
    "\ttraining_len = int(len(array)*split)\n",
    "\ttrainingSet = array[:training_len]\n",
    "\ttestSet = array[training_len:]\n",
    "\treturn trainingSet, testSet\n",
    "\n",
    "\n",
    "def euclideanDistance(instance1, instance2, length):\n",
    "\tdistance = 0\n",
    "\tfor x in range(length):\n",
    "\t\tdistance += pow((instance1[x] - instance2[x]), 2)\n",
    "\treturn math.sqrt(distance)\n",
    "\n",
    "def getNeighbors(trainingSet, testInstance, k):\n",
    "\tdistances = []\n",
    "\tlength = len(testInstance)-1\n",
    "\tfor x in range(len(trainingSet)):\n",
    "\t\tdist = euclideanDistance(testInstance, trainingSet[x], length)\n",
    "\t\tdistances.append((trainingSet[x], dist))\n",
    "\tdistances.sort(key=operator.itemgetter(1))\n",
    "\tneighbors = []\n",
    "\tfor x in range(k):\n",
    "\t\tneighbors.append(distances[x][0])\n",
    "\treturn neighbors\n",
    "\n",
    "def getResponse(neighbors):\n",
    "\tclassVotes = {}\n",
    "\tfor x in range(len(neighbors)):\n",
    "\t\tresponse = neighbors[x][-1]\n",
    "\t\tif response in classVotes:\n",
    "\t\t\tclassVotes[response] += 1\n",
    "\t\telse:\n",
    "\t\t\tclassVotes[response] = 1\n",
    "\tsortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\treturn sortedVotes[0][0]\n",
    "\n",
    "def getAccuracy(testSet, predictions):\n",
    "\tcorrect = 0\n",
    "\tfor x in range(len(testSet)):\n",
    "\t\tif testSet[x][-1] == predictions[x]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn (correct/float(len(testSet))) * 100.0\n",
    "\t\n",
    "def main():\n",
    "\t# prepare data\n",
    "\ttrainingSet=[]\n",
    "\ttestSet=[]\n",
    "\tsplit = 0.67\n",
    "\turl = 'https://raw.githubusercontent.com/ruiwu1990/CSCI_4120/master/KNN/iris.data'\n",
    "\ttrainingSet, testSet = loadDataset(url, 0.66)\n",
    "\tprint('Train set: ' + repr(len(trainingSet)))\n",
    "\tprint('Test set: ' + repr(len(testSet)))\n",
    "\t# generate predictions\n",
    "\tpredictions=[]\n",
    "\tk = 3\n",
    "\t# loop through testSet\n",
    "\tfor x in range(len(testSet)):\n",
    "\t\t# TODO starts here\n",
    "\t\t# get neighor between current test record and all training datasets\n",
    "\t\tneighbors = getNeighbors('''TODO''')\n",
    "\t\t# get response\n",
    "\t\tresult = getResponse('''TODO''')\n",
    "\t\t# append current prediction result to predictions list\n",
    "\t\tpredictions.append('''TODO''')\n",
    "\t\tprint('> predicted=' + repr(result) + ', actual=' + repr(testSet[x][-1]))\n",
    "\t\t# TODO ends here\n",
    "\taccuracy = getAccuracy(testSet, predictions)\n",
    "\tprint('Accuracy: ' + repr(accuracy) + '%')\n",
    "\t\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
